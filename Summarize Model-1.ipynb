{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "teks = \"\"\"\n",
    "Pembangunan infrastruktur di Indonesia sangat penting untuk mendukung pertumbuhan ekonomi. \n",
    "Proyek-proyek seperti jalan tol, jembatan, dan pelabuhan memainkan peran kunci dalam pengembangan wilayah. \n",
    "Dalam beberapa tahun terakhir, pemerintah telah meningkatkan investasi dalam sektor ini. \n",
    "Namun, tantangan seperti anggaran yang terbatas dan kurangnya tenaga ahli tetap menjadi perhatian. \n",
    "Dengan kemajuan teknologi, seperti penggunaan drone dan pencetakan 3D, diharapkan proses pembangunan akan menjadi lebih efisien.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN-Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PoSianturi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text (without punctuation except commas and periods):\n",
      "Teknologi telah mengubah cara berkomunikasi era digital signifikan. Dulu, komunikasi terbatas surat menyurat, telepon, pertemuan tatap muka. Namun, kemunculan internet perangkat mobile, komunikasi kini menjadi cepat mudah. Media sosial memainkan peran penting menyebarkan informasi, memberikan platform bagi individu berbagi pemikiran, berita, pengalaman secara realtime. Banyak orang menghabiskan waktu berjamjam platform Facebook, Instagram, Twitter. Di sana, tidak berinteraksi teman keluarga, juga mengikuti berita terkini tren global. Media sosial telah menjadi sumber informasi utama bagi banyak orang, meskipun sering kali informasi disebarkan tidak selalu akurat terpercaya. Oleh karena itu, penting bagi pengguna selalu kritis memverifikasi informasi sebelum membagikannya. Selain itu, munculnya fenomena hoaks disinformasi media sosial menuntut pengguna cermat memilah informasi. Selain itu, teknologi juga mempengaruhi cara bekerja. Banyak perusahaan kini menerapkan sistem kerja jarak jauh, memungkinkan karyawan bekerja rumah. Ini bukan memberikan kenyamanan, juga memungkinkan fleksibilitas besar mengatur waktu kerja pribadi. Sistem kerja hybrid juga mulai populer, mana karyawan memilih bekerja kantor rumah sesuai kebutuhan mereka. Perubahan membawa sejumlah keuntungan, pengurangan waktu perjalanan peningkatan keseimbangan kehidupan kerja. Namun, perubahan juga membawa tantangan tersendiri, kesulitan berkolaborasi menjaga hubungan antar rekan kerja. Dalam konteks efisiensi, teknologi modern telah membantu banyak perusahaan meningkatkan produktivitas mereka. Alat kolaborasi online, aplikasi manajemen proyek video konferensi, memungkinkan tim bekerja meskipun berada lokasi berbeda. Misalnya, platform Zoom Microsoft Teams memungkinkan rapat virtual mendukung kolaborasi lintas waktu lokasi. Namun, balik semua keuntungan ini, ada tantangan harus dihadapi keamanan data menjadi perhatian utama, terutama meningkatnya ancaman siber. Banyak perusahaan mengalami kebocoran data merugikan, perlindungan informasi menjadi prioritas tidak diabaikan. Oleh karena itu, penting bagi individu perusahaan memahami cara melindungi informasi menggunakan langkahlangkah keamanan tepat, enkripsi data pelatihan keamanan siber bagi karyawan. Di sisi lain, kemajuan teknologi medis memberikan harapan baru. Inovasi pengobatan diagnosis penyakit membantu menyelamatkan nyawa meningkatkan kualitas hidup banyak orang. Dengan adanya teknologi telemedicine, pasien kini berkonsultasi dokter jarak jauh tanpa harus datang langsung rumah sakit. Ini bermanfaat, terutama bagi tinggal daerah terpencil bagi orangorang mobilitas terbatas. Telemedicine memungkinkan akses baik terhadap layanan kesehatan, mengurangi beban sistem kesehatan, mempercepat waktu respon terhadap masalah kesehatan. Di samping itu, penggunaan kecerdasan buatan analisis data medis memungkinkan diagnosis cepat akurat, serta pengembangan perawatan efektif. Misalnya, algoritma AI membantu mengidentifikasi pola data pasien mungkin tidak terlihat dokter. Dengan memanfaatkan machine learning, dokter memperoleh wawasan tentang riwayat kesehatan pasien merancang perawatan personal. Teknologi wearable devices juga memberikan data realtime tentang kesehatan individu, memungkinkan pencegahan penyakit sebelum menjadi masalah serius. Namun, semua perkembangan ini, muncul pertanyaan tentang etika privasi. Bagaimana memastikan bahwa data pribadi dilindungi Siapa bertanggung jawab terjadi kesalahan diagnosis menggunakan teknologi canggih Dengan demikian, teknologi tidak mengubah cara berkomunikasi bekerja, juga berdampak besar kesehatan kesejahteraan kita. Dengan semua perkembangan ini, jelas bahwa teknologi terus memainkan peran sentral kehidupan seharihari kita. Mengadaptasi memahami teknologi baru menjadi kunci menghadapi tantangan memanfaatkan peluang masa depan. Hal tidak berlaku individu, juga organisasi pemerintah perlu beradaptasi cepat terhadap perubahan terjadi. Dengan pendekatan proaktif terhadap teknologi, menciptakan masa depan baik terhubung.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Teknologi telah mengubah cara berkomunikasi era digital signifikan',\n",
       " 'Dulu, komunikasi terbatas surat menyurat, telepon, pertemuan tatap muka',\n",
       " 'Namun, kemunculan internet perangkat mobile, komunikasi kini menjadi cepat mudah',\n",
       " 'Media sosial memainkan peran penting menyebarkan informasi, memberikan platform bagi individu berbagi pemikiran, berita, pengalaman secara realtime',\n",
       " 'Banyak orang menghabiskan waktu berjamjam platform Facebook, Instagram, Twitter',\n",
       " 'Di sana, tidak berinteraksi teman keluarga, juga mengikuti berita terkini tren global',\n",
       " 'Media sosial telah menjadi sumber informasi utama bagi banyak orang, meskipun sering kali informasi disebarkan tidak selalu akurat terpercaya',\n",
       " 'Oleh karena itu, penting bagi pengguna selalu kritis memverifikasi informasi sebelum membagikannya',\n",
       " 'Selain itu, munculnya fenomena hoaks disinformasi media sosial menuntut pengguna cermat memilah informasi',\n",
       " 'Selain itu, teknologi juga mempengaruhi cara bekerja',\n",
       " 'Banyak perusahaan kini menerapkan sistem kerja jarak jauh, memungkinkan karyawan bekerja rumah',\n",
       " 'Ini bukan memberikan kenyamanan, juga memungkinkan fleksibilitas besar mengatur waktu kerja pribadi',\n",
       " 'Sistem kerja hybrid juga mulai populer, mana karyawan memilih bekerja kantor rumah sesuai kebutuhan mereka',\n",
       " 'Perubahan membawa sejumlah keuntungan, pengurangan waktu perjalanan peningkatan keseimbangan kehidupan kerja',\n",
       " 'Namun, perubahan juga membawa tantangan tersendiri, kesulitan berkolaborasi menjaga hubungan antar rekan kerja',\n",
       " 'Dalam konteks efisiensi, teknologi modern telah membantu banyak perusahaan meningkatkan produktivitas mereka',\n",
       " 'Alat kolaborasi online, aplikasi manajemen proyek video konferensi, memungkinkan tim bekerja meskipun berada lokasi berbeda',\n",
       " 'Misalnya, platform Zoom Microsoft Teams memungkinkan rapat virtual mendukung kolaborasi lintas waktu lokasi',\n",
       " 'Namun, balik semua keuntungan ini, ada tantangan harus dihadapi keamanan data menjadi perhatian utama, terutama meningkatnya ancaman siber',\n",
       " 'Banyak perusahaan mengalami kebocoran data merugikan, perlindungan informasi menjadi prioritas tidak diabaikan',\n",
       " 'Oleh karena itu, penting bagi individu perusahaan memahami cara melindungi informasi menggunakan langkahlangkah keamanan tepat, enkripsi data pelatihan keamanan siber bagi karyawan',\n",
       " 'Di sisi lain, kemajuan teknologi medis memberikan harapan baru',\n",
       " 'Inovasi pengobatan diagnosis penyakit membantu menyelamatkan nyawa meningkatkan kualitas hidup banyak orang',\n",
       " 'Dengan adanya teknologi telemedicine, pasien kini berkonsultasi dokter jarak jauh tanpa harus datang langsung rumah sakit',\n",
       " 'Ini bermanfaat, terutama bagi tinggal daerah terpencil bagi orangorang mobilitas terbatas',\n",
       " 'Telemedicine memungkinkan akses baik terhadap layanan kesehatan, mengurangi beban sistem kesehatan, mempercepat waktu respon terhadap masalah kesehatan',\n",
       " 'Di samping itu, penggunaan kecerdasan buatan analisis data medis memungkinkan diagnosis cepat akurat, serta pengembangan perawatan efektif',\n",
       " 'Misalnya, algoritma AI membantu mengidentifikasi pola data pasien mungkin tidak terlihat dokter',\n",
       " 'Dengan memanfaatkan machine learning, dokter memperoleh wawasan tentang riwayat kesehatan pasien merancang perawatan personal',\n",
       " 'Teknologi wearable devices juga memberikan data realtime tentang kesehatan individu, memungkinkan pencegahan penyakit sebelum menjadi masalah serius',\n",
       " 'Namun, semua perkembangan ini, muncul pertanyaan tentang etika privasi',\n",
       " 'Bagaimana memastikan bahwa data pribadi dilindungi Siapa bertanggung jawab terjadi kesalahan diagnosis menggunakan teknologi canggih Dengan demikian, teknologi tidak mengubah cara berkomunikasi bekerja, juga berdampak besar kesehatan kesejahteraan kita',\n",
       " 'Dengan semua perkembangan ini, jelas bahwa teknologi terus memainkan peran sentral kehidupan seharihari kita',\n",
       " 'Mengadaptasi memahami teknologi baru menjadi kunci menghadapi tantangan memanfaatkan peluang masa depan',\n",
       " 'Hal tidak berlaku individu, juga organisasi pemerintah perlu beradaptasi cepat terhadap perubahan terjadi',\n",
       " 'Dengan pendekatan proaktif terhadap teknologi, menciptakan masa depan baik terhubung']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "class CleaningData:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        \n",
    "    def clean_text(self):\n",
    "        # Daftar stop words\n",
    "        stop_words_list = set([\n",
    "            \"yang\", \"di\", \"dan\", \"untuk\", \"dari\", \"ke\", \"dalam\", \"adalah\", \n",
    "            \"ini\", \"itu\", \"pada\", \"atau\", \"dapat\", \"sebuah\", \"kita\", \"anda\", \n",
    "            \"tersebut\", \"oleh\", \"dengan\", \"seperti\", \"jika\", \"tetapi\", \"saja\", \n",
    "            \"apa\", \"lebih\", \"sama\", \"setiap\", \"mereka\", \"akan\", \"sehingga\", \n",
    "            \"hanya\", \"namun\", \"di mana\", \"bisa\", \"saat\", \"sangat\", \"hal\"\n",
    "        ])\n",
    "        \n",
    "        # Menghapus tanda baca kecuali koma dan titik\n",
    "        text_no_other_punctuation = self.text.translate(str.maketrans('', '', string.punctuation.replace(',', '').replace('.', '')))\n",
    "        \n",
    "        # Menghapus stop words\n",
    "        filtered_words = [\n",
    "            word for word in text_no_other_punctuation.split() if word not in stop_words_list\n",
    "        ]\n",
    "        \n",
    "        # Menggabungkan kembali kata\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "# Contoh penggunaan\n",
    "text = \"\"\"\n",
    "Teknologi telah mengubah cara kita berkomunikasi di era digital ini dengan sangat signifikan. Dulu, komunikasi terbatas pada surat menyurat, telepon, dan pertemuan tatap muka. Namun, dengan kemunculan internet dan perangkat mobile, komunikasi kini menjadi lebih cepat dan mudah. Media sosial memainkan peran penting dalam menyebarkan informasi, memberikan platform bagi individu untuk berbagi pemikiran, berita, dan pengalaman mereka secara real-time.\n",
    "\n",
    "Banyak orang menghabiskan waktu berjam-jam di platform seperti Facebook, Instagram, dan Twitter. Di sana, mereka tidak hanya berinteraksi dengan teman dan keluarga, tetapi juga mengikuti berita terkini dan tren global. Media sosial telah menjadi sumber informasi utama bagi banyak orang, meskipun sering kali informasi yang disebarkan tidak selalu akurat atau terpercaya. Oleh karena itu, penting bagi pengguna untuk selalu kritis dan memverifikasi informasi sebelum membagikannya. Selain itu, munculnya fenomena \"hoaks\" dan disinformasi di media sosial menuntut pengguna untuk lebih cermat dalam memilah informasi.\n",
    "\n",
    "Selain itu, teknologi juga mempengaruhi cara kita bekerja. Banyak perusahaan kini menerapkan sistem kerja jarak jauh, memungkinkan karyawan untuk bekerja dari rumah. Ini bukan hanya memberikan kenyamanan, tetapi juga memungkinkan fleksibilitas yang lebih besar dalam mengatur waktu kerja dan pribadi. Sistem kerja hybrid juga mulai populer, di mana karyawan dapat memilih untuk bekerja dari kantor atau dari rumah sesuai kebutuhan mereka. Perubahan ini membawa sejumlah keuntungan, seperti pengurangan waktu perjalanan dan peningkatan keseimbangan kehidupan kerja. Namun, perubahan ini juga membawa tantangan tersendiri, seperti kesulitan dalam berkolaborasi dan menjaga hubungan antar rekan kerja.\n",
    "\n",
    "Dalam konteks efisiensi, teknologi modern telah membantu banyak perusahaan untuk meningkatkan produktivitas mereka. Alat kolaborasi online, seperti aplikasi manajemen proyek dan video konferensi, memungkinkan tim untuk bekerja sama meskipun berada di lokasi yang berbeda. Misalnya, platform seperti Zoom dan Microsoft Teams memungkinkan rapat virtual yang mendukung kolaborasi lintas waktu dan lokasi. Namun, di balik semua keuntungan ini, ada tantangan yang harus dihadapi; keamanan data menjadi perhatian utama, terutama dengan meningkatnya ancaman siber. Banyak perusahaan mengalami kebocoran data yang merugikan, dan perlindungan informasi menjadi prioritas yang tidak bisa diabaikan. Oleh karena itu, penting bagi individu dan perusahaan untuk memahami cara melindungi informasi mereka dengan menggunakan langkah-langkah keamanan yang tepat, seperti enkripsi data dan pelatihan keamanan siber bagi karyawan.\n",
    "\n",
    "Di sisi lain, kemajuan dalam teknologi medis memberikan harapan baru. Inovasi dalam pengobatan dan diagnosis penyakit membantu menyelamatkan nyawa dan meningkatkan kualitas hidup banyak orang. Dengan adanya teknologi seperti telemedicine, pasien kini dapat berkonsultasi dengan dokter dari jarak jauh tanpa harus datang langsung ke rumah sakit. Ini sangat bermanfaat, terutama bagi mereka yang tinggal di daerah terpencil atau bagi orang-orang dengan mobilitas terbatas. Telemedicine memungkinkan akses yang lebih baik terhadap layanan kesehatan, mengurangi beban pada sistem kesehatan, dan mempercepat waktu respon terhadap masalah kesehatan.\n",
    "\n",
    "Di samping itu, penggunaan kecerdasan buatan dalam analisis data medis memungkinkan diagnosis yang lebih cepat dan akurat, serta pengembangan perawatan yang lebih efektif. Misalnya, algoritma AI dapat membantu dalam mengidentifikasi pola dalam data pasien yang mungkin tidak terlihat oleh dokter. Dengan memanfaatkan machine learning, dokter dapat memperoleh wawasan yang lebih dalam tentang riwayat kesehatan pasien dan merancang perawatan yang lebih personal. Teknologi seperti wearable devices juga memberikan data real-time tentang kesehatan individu, sehingga memungkinkan pencegahan penyakit sebelum menjadi masalah serius.\n",
    "\n",
    "Namun, dengan semua perkembangan ini, muncul pertanyaan tentang etika dan privasi. Bagaimana kita memastikan bahwa data pribadi kita dilindungi? Siapa yang bertanggung jawab jika terjadi kesalahan dalam diagnosis yang menggunakan teknologi canggih? Dengan demikian, teknologi tidak hanya mengubah cara kita berkomunikasi dan bekerja, tetapi juga berdampak besar pada kesehatan dan kesejahteraan kita.\n",
    "\n",
    "Dengan semua perkembangan ini, sangat jelas bahwa teknologi akan terus memainkan peran sentral dalam kehidupan sehari-hari kita. Mengadaptasi dan memahami teknologi baru menjadi kunci untuk menghadapi tantangan dan memanfaatkan peluang di masa depan. Hal ini tidak hanya berlaku untuk individu, tetapi juga untuk organisasi dan pemerintah yang perlu beradaptasi dengan cepat terhadap perubahan yang terjadi. Dengan pendekatan yang proaktif terhadap teknologi, kita dapat menciptakan masa depan yang lebih baik dan lebih terhubung.\n",
    "\"\"\"\n",
    "\n",
    "cleaner = CleaningData(text)\n",
    "cleaned_text = cleaner.clean_text()\n",
    "\n",
    "print(\"Cleaned text (without punctuation except commas and periods):\")\n",
    "print(cleaned_text)\n",
    "\n",
    "# Mengubah teks yang sudah dibersihkan menjadi satu list, di mana setiap kalimat menjadi elemen\n",
    "cleaned_sentences = [sentence.strip() for sentence in cleaned_text.split('.') if sentence.strip()]\n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFID\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PoSianturi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'xx_ent_wiki_sm' (3.7.0) was trained with spaCy v3.7.0 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_add_pool, global_max_pool\n",
    "import networkx as nx\n",
    "import spacy\n",
    "\n",
    "# Load multilingual model for spaCy\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "# Add the sentencizer component\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class CountVectorizer:\n",
    "    def generate_vocabulary(self):\n",
    "        self.vocabulary = set()\n",
    "        for sentence in self.corpus:\n",
    "            word_split = sentence.lower().split(' ')\n",
    "            for word in word_split:\n",
    "                self.vocabulary.add(word)\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.generate_vocabulary()\n",
    "        self.word_idx = {word: idx for idx, word in enumerate(self.vocabulary)}\n",
    "        self.idx_word = {idx: word for idx, word in enumerate(self.vocabulary)}\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocabulary\n",
    "\n",
    "    def get_mapping(self):\n",
    "        return self.word_idx, self.idx_word\n",
    "\n",
    "    def transform(self, text):\n",
    "        text_features = np.zeros(shape=(len(text), len(self.vocabulary)))\n",
    "        for idx_sentence, sentence in enumerate(text):\n",
    "            for word in sentence.lower().split(' '):\n",
    "                if word not in self.word_idx:\n",
    "                    continue\n",
    "                word_index = self.word_idx[word]\n",
    "                text_features[idx_sentence, word_index] += 1\n",
    "        return text_features\n",
    "\n",
    "\n",
    "class TFIDFVectorizer:\n",
    "    def generate_vocabulary(self):\n",
    "        self.vocabulary = set()\n",
    "        for sentence in self.corpus:\n",
    "            word_split = sentence.lower().split(' ')\n",
    "            for word in word_split:\n",
    "                self.vocabulary.add(word)\n",
    "\n",
    "    def generate_number_word_mention(self):\n",
    "        self.word_mention = {}\n",
    "        for sentence in self.corpus:\n",
    "            for word in set(sentence.lower().split(' ')):\n",
    "                self.word_mention[word] = self.word_mention.get(word, 0) + 1\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        self.corpus = corpus\n",
    "        self.generate_vocabulary()\n",
    "        self.generate_number_word_mention()\n",
    "        self.word_idx = {word: idx for idx, word in enumerate(self.vocabulary)}\n",
    "        self.idx_word = {idx: word for idx, word in enumerate(self.vocabulary)}\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocabulary\n",
    "\n",
    "    def get_mapping(self):\n",
    "        return self.word_idx, self.idx_word\n",
    "\n",
    "    def transform(self, text, threshold=0):\n",
    "        text_features = np.zeros(shape=(len(text), len(self.vocabulary)))\n",
    "        for idx_sentence, sentence in enumerate(text):\n",
    "            word_count = Counter(sentence.lower().split(' '))\n",
    "            for word in sentence.lower().split(' '):\n",
    "                if word not in self.word_idx:\n",
    "                    continue\n",
    "                word_count_in_sentence = word_count[word]\n",
    "                tf = word_count_in_sentence / len(sentence.lower().split(' '))\n",
    "                number_word_mention = self.word_mention[word]\n",
    "                idf = np.log(len(self.corpus) / number_word_mention) if number_word_mention > 0 else 0\n",
    "                tfidf = tf * idf\n",
    "                text_features[idx_sentence, self.word_idx[word]] = tfidf\n",
    "\n",
    "        # Mengambil kata-kata penting berdasarkan threshold\n",
    "        important_words = {\n",
    "            idx_sentence: [\n",
    "                (self.idx_word[idx], score)\n",
    "                for idx, score in enumerate(text_features[idx_sentence])\n",
    "                if score > threshold\n",
    "            ]\n",
    "            for idx_sentence in range(len(text_features))\n",
    "        }\n",
    "\n",
    "        # Mengambil kata-kata dengan skor tertinggi secara keseluruhan\n",
    "        overall_scores = np.sum(text_features, axis=0)\n",
    "        overall_important_words = [\n",
    "            (self.idx_word[idx], overall_scores[idx])\n",
    "            for idx in range(len(overall_scores))\n",
    "            if overall_scores[idx] > threshold\n",
    "        ]\n",
    "\n",
    "        return important_words, overall_important_words\n",
    "    \n",
    "class SimpleGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu', aggregation='mean'):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "\n",
    "        # Layer pertama: GCN\n",
    "        self.conv1 = GCNConv(in_channels, 16)\n",
    "\n",
    "        # Layer kedua: GAT\n",
    "        self.conv2 = GATConv(16, 16, heads=1)\n",
    "\n",
    "        # Layer ketiga: GAT\n",
    "        self.conv3 = GATConv(16, 16, heads=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.final_conv = GCNConv(16, out_channels)\n",
    "\n",
    "        # Menyimpan jenis aktivasi dan agregasi\n",
    "        self.activation = activation\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Layer pertama\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Layer kedua\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Layer ketiga\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Fungsi agregasi global\n",
    "        if self.aggregation == 'mean':\n",
    "            h = global_mean_pool(h, batch)\n",
    "        elif self.aggregation == 'sum':\n",
    "            h = global_add_pool(h, batch)\n",
    "        elif self.aggregation == 'max':\n",
    "            h = global_max_pool(h, batch)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation type. Choose 'mean', 'sum', or 'max'.\")\n",
    "\n",
    "        # Output\n",
    "        h = self.final_conv(h, edge_index)\n",
    "        return h\n",
    "\n",
    "    def _apply_activation(self, h):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(h)\n",
    "        elif self.activation == 'tanh':\n",
    "            return F.tanh(h)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation type. Choose 'relu' or 'tanh'.\")\n",
    "\n",
    "class TextSummarizer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_sentences(text):\n",
    "        return [sent.text.strip() for sent in nlp(text).sents]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_entities(text):\n",
    "        doc = nlp(text)\n",
    "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_entity(entity):\n",
    "        return entity.lower().strip()\n",
    "\n",
    "    def build_graph(self, sentences):\n",
    "        graph = nx.Graph()\n",
    "        entity_map = {}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Call extract_entities on the current sentence\n",
    "            entities = self.extract_entities(sentence) \n",
    "            if not entities:\n",
    "                continue\n",
    "\n",
    "            normalized_entities = [(self.normalize_entity(ent[0]), ent[1]) for ent in entities]\n",
    "\n",
    "            for entity in normalized_entities:\n",
    "                graph.add_node(entity[0])\n",
    "                if entity[0] in entity_map:\n",
    "                    entity_map[entity[0]].add(sentence)\n",
    "                else:\n",
    "                    entity_map[entity[0]] = {sentence}\n",
    "\n",
    "            for i in range(len(normalized_entities)):\n",
    "                for j in range(i + 1, len(normalized_entities)):\n",
    "                    graph.add_edge(normalized_entities[i][0], normalized_entities[j][0])\n",
    "\n",
    "        for entity, sentences in entity_map.items():\n",
    "            if len(sentences) > 1:\n",
    "                combined_node = f\"combined_{entity}\"\n",
    "                graph.add_node(combined_node)\n",
    "                for sentence in sentences:\n",
    "                    for ent in entities:\n",
    "                        if self.normalize_entity(ent[0]) == entity:\n",
    "                            graph.add_edge(combined_node, ent[0])\n",
    "                graph = nx.relabel_nodes(graph, {entity: combined_node})\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def summarizer(self, text, num_sentences=5, graph_algorithm='rank', threshold=0.1):\n",
    "        sentences = self.extract_sentences(text)\n",
    "        graph = self.build_graph(sentences)\n",
    "\n",
    "        if graph.number_of_nodes() == 0:\n",
    "            return [], \"\"\n",
    "\n",
    "        sentence_scores = {sentence: 0 for sentence in sentences}\n",
    "\n",
    "        if graph_algorithm == 'rank':\n",
    "            scores = nx.pagerank(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(scores.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        elif graph_algorithm == 'closeness':\n",
    "            scores = nx.degree_centrality(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = scores.get(sentence, 0)\n",
    "\n",
    "        elif graph_algorithm == 'hits':\n",
    "            hubs, authorities = nx.hits(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(authorities.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        elif graph_algorithm == 'degree':\n",
    "            scores = dict(graph.degree())\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(scores.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph_algorithm. Use 'rank', 'closeness', 'hits', or 'degree'.\")\n",
    "\n",
    "        ranked_sentences = sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        selected_sentences = []\n",
    "        selected_indices = set()\n",
    "\n",
    "        for sentence, score in ranked_sentences:\n",
    "            idx = sentences.index(sentence)\n",
    "            if not any(abs(idx - si) < 2 for si in selected_indices):\n",
    "                selected_sentences.append(sentence)\n",
    "                selected_indices.add(idx)\n",
    "                if len(selected_sentences) == num_sentences:\n",
    "                    break\n",
    "\n",
    "        # Create TFIDFVectorizer and fit it on the selected sentences\n",
    "        tfidf_vectorizer = TFIDFVectorizer()\n",
    "        tfidf_vectorizer.fit(selected_sentences)\n",
    "\n",
    "        # Set threshold\n",
    "        threshold = threshold\n",
    "        important_words, overall_important_words = tfidf_vectorizer.transform(selected_sentences, threshold)\n",
    "\n",
    "        # Store important words into a list\n",
    "        important_words_list = []\n",
    "        for idx_sentence, words in important_words.items():\n",
    "            sentence_importance = []\n",
    "            for word, score in words:\n",
    "                sentence_importance.append((word, score))\n",
    "            important_words_list.append((idx_sentence + 1, sentence_importance))\n",
    "        \n",
    "        # Join important words into a single string\n",
    "        important_words_string = ' '.join(word for _, words in important_words_list for word, _ in words)\n",
    "\n",
    "        return important_words_list, important_words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tersendiri, kesulitan rekan perubahan antar namun, berkolaborasi menjaga tantangan kerja. membawa hubungan memilih mulai sistem mereka. populer, kebutuhan hybrid mana kerja karyawan sesuai kantor rumah berbeda. berada aplikasi tim lokasi alat proyek video online, manajemen kolaborasi memungkinkan konferensi, keamanan'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming cleaned_text is your input text\n",
    "summarizer_tfid = TextSummarizer()\n",
    "score_important, ringkasan_tfid = summarizer_tfid.summarizer(cleaned_text)\n",
    "\n",
    "ringkasan_tfid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_add_pool, global_max_pool\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load multilingual model for spaCy\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "# Add the sentencizer component\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "class SimpleGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu', aggregation='mean'):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "\n",
    "        # Layer pertama: GCN\n",
    "        self.conv1 = GCNConv(in_channels, 16)\n",
    "\n",
    "        # Layer kedua: GAT\n",
    "        self.conv2 = GATConv(16, 16, heads=1)\n",
    "\n",
    "        # Layer ketiga: GAT\n",
    "        self.conv3 = GATConv(16, 16, heads=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.final_conv = GCNConv(16, out_channels)\n",
    "\n",
    "        # Menyimpan jenis aktivasi dan agregasi\n",
    "        self.activation = activation\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Layer pertama\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Layer kedua\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Layer ketiga\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Fungsi agregasi global\n",
    "        if self.aggregation == 'mean':\n",
    "            h = global_mean_pool(h, batch)\n",
    "        elif self.aggregation == 'sum':\n",
    "            h = global_add_pool(h, batch)\n",
    "        elif self.aggregation == 'max':\n",
    "            h = global_max_pool(h, batch)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation type. Choose 'mean', 'sum', or 'max'.\")\n",
    "\n",
    "        # Output\n",
    "        h = self.final_conv(h, edge_index)\n",
    "        return h\n",
    "\n",
    "    def _apply_activation(self, h):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(h)\n",
    "        elif self.activation == 'tanh':\n",
    "            return F.tanh(h)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation type. Choose 'relu' or 'tanh'.\")\n",
    "\n",
    "class TextSummarizer:\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_sentences(text):\n",
    "        return [sent.text.strip() for sent in nlp(text).sents]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_entities(text):\n",
    "        doc = nlp(text)\n",
    "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_entity(entity):\n",
    "        return entity.lower().strip()\n",
    "\n",
    "    def build_graph(self, sentences):\n",
    "        graph = nx.Graph()\n",
    "        entity_map = {}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            entities = self.extract_entities(sentence)\n",
    "            if not entities:\n",
    "                continue\n",
    "\n",
    "            normalized_entities = [(self.normalize_entity(ent[0]), ent[1]) for ent in entities]\n",
    "\n",
    "            for entity in normalized_entities:\n",
    "                graph.add_node(entity[0])\n",
    "                if entity[0] in entity_map:\n",
    "                    entity_map[entity[0]].add(sentence)\n",
    "                else:\n",
    "                    entity_map[entity[0]] = {sentence}\n",
    "\n",
    "            for i in range(len(normalized_entities)):\n",
    "                for j in range(i + 1, len(normalized_entities)):\n",
    "                    graph.add_edge(normalized_entities[i][0], normalized_entities[j][0])\n",
    "\n",
    "        for entity, sentences in entity_map.items():\n",
    "            if len(sentences) > 1:\n",
    "                combined_node = f\"combined_{entity}\"\n",
    "                graph.add_node(combined_node)\n",
    "                for sentence in sentences:\n",
    "                    for ent in entities:\n",
    "                        if self.normalize_entity(ent[0]) == entity:\n",
    "                            graph.add_edge(combined_node, ent[0])\n",
    "                graph = nx.relabel_nodes(graph, {entity: combined_node})\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def summarizer(self, text, num_sentences=5, graph_algorithm='rank'):\n",
    "        sentences = self.extract_sentences(text)\n",
    "        graph = self.build_graph(sentences)\n",
    "\n",
    "        if graph.number_of_nodes() == 0:\n",
    "            return [], \"\"\n",
    "\n",
    "        sentence_scores = {sentence: 0 for sentence in sentences}\n",
    "\n",
    "        if graph_algorithm == 'rank':\n",
    "            scores = nx.pagerank(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(scores.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        elif graph_algorithm == 'closeness':\n",
    "            scores = nx.degree_centrality(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = scores.get(sentence, 0)\n",
    "\n",
    "        elif graph_algorithm == 'hits':\n",
    "            hubs, authorities = nx.hits(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(authorities.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        elif graph_algorithm == 'degree':\n",
    "            scores = dict(graph.degree())\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(scores.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph_algorithm. Use 'rank', 'closeness', 'hits', or 'degree'.\")\n",
    "\n",
    "        ranked_sentences = sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        selected_sentences = []\n",
    "        selected_indices = set()\n",
    "\n",
    "        for sentence, score in ranked_sentences:\n",
    "            idx = sentences.index(sentence)\n",
    "            if not any(abs(idx - si) < 2 for si in selected_indices):\n",
    "                selected_sentences.append(sentence)\n",
    "                selected_indices.add(idx)\n",
    "                if len(selected_sentences) == num_sentences:\n",
    "                    break\n",
    "\n",
    "        # Create Word2Vec model\n",
    "        word2vec_model = Word2Vec(sentences=[sentence.split() for sentence in selected_sentences], vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "        # Extract important words using Word2Vec embeddings\n",
    "        important_words_list = []\n",
    "        for sentence in selected_sentences:\n",
    "            words = sentence.split()\n",
    "            sentence_importance = [(word, word2vec_model.wv[word]) for word in words if word in word2vec_model.wv]\n",
    "            important_words_list.append((sentence, sentence_importance))\n",
    "\n",
    "        # Create a string of important words\n",
    "        important_words_string = ' '.join(word for _, words in important_words_list for word, _ in words)\n",
    "\n",
    "        return important_words_list, important_words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Namun, perubahan juga membawa tantangan tersendiri, kesulitan berkolaborasi menjaga hubungan antar rekan kerja. Media sosial telah menjadi sumber informasi utama bagi banyak orang, meskipun sering kali informasi disebarkan tidak selalu akurat terpercaya. Sistem kerja hybrid juga mulai populer, mana karyawan memilih bekerja kantor rumah sesuai kebutuhan mereka. Alat kolaborasi online, aplikasi manajemen proyek video konferensi, memungkinkan tim bekerja meskipun berada lokasi berbeda. Oleh karena itu, penting bagi individu perusahaan memahami cara melindungi informasi menggunakan langkahlangkah keamanan tepat, enkripsi data pelatihan keamanan siber bagi karyawan.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming cleaned_text is your input text\n",
    "summarizer_wo2vec = TextSummarizer()\n",
    "score_important, ringkasan_wo2vec = summarizer_wo2vec.summarizer(cleaned_text)\n",
    "\n",
    "ringkasan_wo2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_add_pool, global_max_pool\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Load multilingual model for spaCy\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "class SimpleGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu', aggregation='mean'):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        \n",
    "        # Layer pertama: GCN\n",
    "        self.conv1 = GCNConv(in_channels, 16)\n",
    "        \n",
    "        # Layer kedua: GAT\n",
    "        self.conv2 = GATConv(16, 16, heads=1)\n",
    "        \n",
    "        # Layer ketiga: GAT\n",
    "        self.conv3 = GATConv(16, 16, heads=1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.final_conv = GCNConv(16, out_channels)\n",
    "        \n",
    "        # Menyimpan jenis aktivasi dan agregasi\n",
    "        self.activation = activation\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Layer pertama\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Layer kedua\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Layer ketiga\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Fungsi agregasi global\n",
    "        if self.aggregation == 'mean':\n",
    "            h = global_mean_pool(h, batch)\n",
    "        elif self.aggregation == 'sum':\n",
    "            h = global_add_pool(h, batch)\n",
    "        elif self.aggregation == 'max':\n",
    "            h = global_max_pool(h, batch)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation type. Choose 'mean', 'sum', or 'max'.\")\n",
    "\n",
    "        # Output\n",
    "        h = self.final_conv(h, edge_index)\n",
    "        return h\n",
    "\n",
    "    def _apply_activation(self, h):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(h)\n",
    "        elif self.activation == 'tanh':\n",
    "            return F.tanh(h)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation type. Choose 'relu' or 'tanh'.\")\n",
    "\n",
    "class TextSummarizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_sentences(text):\n",
    "        return [sent.text.strip() for sent in nlp(text).sents]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_entities(text):\n",
    "        doc = nlp(text)\n",
    "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_entity(entity):\n",
    "        return entity.lower().strip()\n",
    "\n",
    "    def build_graph(self, sentences):\n",
    "        graph = nx.Graph()\n",
    "        entity_map = {}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            entities = self.extract_entities(sentence)\n",
    "            if not entities:\n",
    "                continue\n",
    "\n",
    "            normalized_entities = [(self.normalize_entity(ent[0]), ent[1]) for ent in entities]\n",
    "\n",
    "            # Add node for the sentence\n",
    "            graph.add_node(sentence, type='sentence')\n",
    "\n",
    "            for entity in normalized_entities:\n",
    "                # Add node for the entity\n",
    "                graph.add_node(entity[0], type='entity')\n",
    "\n",
    "                # Add edge between sentence and entity\n",
    "                graph.add_edge(sentence, entity[0], type='contains')\n",
    "\n",
    "                if entity[0] in entity_map:\n",
    "                    entity_map[entity[0]].add(sentence)\n",
    "                else:\n",
    "                    entity_map[entity[0]] = {sentence}\n",
    "\n",
    "            # Connect entities to each other\n",
    "            for i in range(len(normalized_entities)):\n",
    "                for j in range(i + 1, len(normalized_entities)):\n",
    "                    graph.add_edge(normalized_entities[i][0], normalized_entities[j][0], type='related')\n",
    "\n",
    "            # Add syntactic relationships\n",
    "            doc = nlp(sentence)\n",
    "            for token in doc:\n",
    "                if token.dep_ in {'nsubj', 'dobj', 'pobj'}:\n",
    "                    graph.add_edge(sentence, token.text, type='syntactic')\n",
    "\n",
    "                # Identify and connect temporal entities\n",
    "                if token.ent_type_ in {'DATE', 'TIME', 'EVENT'}:  # Assuming these cover your temporal entities\n",
    "                    graph.add_node(token.text, type='temporal')\n",
    "                    graph.add_edge(sentence, token.text, type='temporal')\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def extract_bert_embeddings(self, sentences):\n",
    "        embeddings = []\n",
    "        for sentence in sentences:\n",
    "            inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state[0][0].numpy())\n",
    "        return embeddings\n",
    "\n",
    "    def select_important_words(self, selected_sentences, embeddings):\n",
    "        important_words = []\n",
    "        for i, sentence in enumerate(selected_sentences):\n",
    "            sentence_embedding = embeddings[i]\n",
    "            words = sentence.split()\n",
    "            word_embeddings = self.extract_bert_embeddings(words)\n",
    "\n",
    "            distances = [np.linalg.norm(word_emb - sentence_embedding) for word_emb in word_embeddings]\n",
    "            closest_word_index = np.argmin(distances)\n",
    "            important_words.append(words[closest_word_index])\n",
    "            \n",
    "        return important_words\n",
    "\n",
    "    def summarizer(self, text, num_sentences=5, graph_algorithm='rank'):\n",
    "        sentences = self.extract_sentences(text)\n",
    "        graph = self.build_graph(sentences)\n",
    "\n",
    "        if graph.number_of_nodes() == 0:\n",
    "            return [], \"\"\n",
    "\n",
    "        embeddings = self.extract_bert_embeddings(sentences)\n",
    "        sentence_scores = {sentence: 0 for sentence in sentences}\n",
    "\n",
    "        if graph_algorithm == 'rank':\n",
    "            scores = nx.pagerank(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(scores.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        elif graph_algorithm == 'closeness':\n",
    "            scores = nx.degree_centrality(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = scores.get(sentence, 0)\n",
    "\n",
    "        elif graph_algorithm == 'hits':\n",
    "            hubs, authorities = nx.hits(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(authorities.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        elif graph_algorithm == 'degree':\n",
    "            scores = dict(graph.degree())\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(scores.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph_algorithm. Use 'rank', 'closeness', 'hits', or 'degree'.\")\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            embedding_score = np.linalg.norm(embeddings[i])\n",
    "            sentence_scores[sentence] += embedding_score * 0.1\n",
    "\n",
    "        ranked_sentences = sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        selected_sentences = []\n",
    "        selected_indices = set()\n",
    "\n",
    "        for sentence, score in ranked_sentences:\n",
    "            idx = sentences.index(sentence)\n",
    "            if not any(abs(idx - si) < 2 for si in selected_indices):\n",
    "                selected_sentences.append(sentence)\n",
    "                selected_indices.add(idx)\n",
    "                if len(selected_sentences) == num_sentences:\n",
    "                    break\n",
    "\n",
    "        important_words = self.select_important_words(selected_sentences, embeddings)\n",
    "\n",
    "        # Return hanya kalimat terpilih dan kata penting\n",
    "        return selected_sentences, important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming cleaned_text is your input text\n",
    "summarizer_bert = TextSummarizer()\n",
    "selected_sentences, important_words = summarizer_bert.summarizer(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Telemedicine memungkinkan akses baik terhadap layanan kesehatan, mengurangi beban sistem kesehatan, mempercepat waktu respon terhadap masalah kesehatan.',\n",
       " 'Dengan memanfaatkan machine learning, dokter memperoleh wawasan tentang riwayat kesehatan pasien merancang perawatan personal.',\n",
       " 'Media sosial telah menjadi sumber informasi utama bagi banyak orang, meskipun sering kali informasi disebarkan tidak selalu akurat terpercaya.',\n",
       " 'Sistem kerja hybrid juga mulai populer, mana karyawan memilih bekerja kantor rumah sesuai kebutuhan mereka.',\n",
       " 'Selain itu, munculnya fenomena hoaks disinformasi media sosial menuntut pengguna cermat memilah informasi.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Telemedicine memungkinkan akses baik terhadap layanan kesehatan, mengurangi beban sistem kesehatan, mempercepat waktu respon terhadap masalah kesehatan.',\n",
    " 'Dengan memanfaatkan machine learning, dokter memperoleh wawasan tentang riwayat kesehatan pasien merancang perawatan personal.',\n",
    " 'Media sosial telah menjadi sumber informasi utama bagi banyak orang, meskipun sering kali informasi disebarkan tidak selalu akurat terpercaya.',\n",
    " 'Sistem kerja hybrid juga mulai populer, mana karyawan memilih bekerja kantor rumah sesuai kebutuhan mereka.',\n",
    " 'Selain itu, munculnya fenomena hoaks disinformasi media sosial menuntut pengguna cermat memilah informasi.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kesehatan.', 'pasien', 'disebarkan', 'populer,', 'disinformasi']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similatiry\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_score(text1, text2):\n",
    "    '''Calculate Cosine Similarity between two texts.'''\n",
    "    # Pastikan input adalah string\n",
    "    text1 = ' '.join(text1) if isinstance(text1, list) else text1\n",
    "    text2 = ' '.join(text2) if isinstance(text2, list) else text2\n",
    "\n",
    "    # Convert both texts into vectors using CountVectorizer\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Calculate and return the cosine similarity between the two vectors\n",
    "    cosine_sim = cosine_similarity(vectors)\n",
    "    return cosine_sim[0][1]  # Mengembalikan nilai kesamaan antara dua teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Tfid 0.30592353823088514\n",
      "Cosine Word2vec 0.6099810859245831\n",
      "Cosine Bert 0.6099810859245831\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity_score\n",
    "cosine_tfid = cosine_similarity_score(cleaned_sentences, ringkasan_tfid)\n",
    "cosine_wo2vec = cosine_similarity_score(cleaned_sentences, ringkasan_wo2vec)\n",
    "cosine_bert = cosine_similarity_score(cleaned_sentences, ringkasan_bert)\n",
    "\n",
    "print('Cosine Tfid', cosine_tfid)\n",
    "print('Cosine Word2vec', cosine_wo2vec)\n",
    "print('Cosine Bert', cosine_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overlap\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Precision = Count(gramn) / Countmatch(gramn)\n",
    "- Recall = Count(gramn) dari referensi Countmatch(gramn)\n",
    "\n",
    "\n",
    "\n",
    "​\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "cleaned_sentences = \" \".join(cleaned_sentences) \n",
    "cleaned_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.0807\n",
      "  F1 Score: 0.1494\n",
      "ROUGE-2:\n",
      "  Precision: 0.0789\n",
      "  Recall: 0.0062\n",
      "  F1 Score: 0.0115\n",
      "ROUGE-L:\n",
      "  Precision: 0.4359\n",
      "  Recall: 0.0352\n",
      "  F1 Score: 0.0651\n"
     ]
    }
   ],
   "source": [
    "# Membuat objek scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Menghitung skor\n",
    "scores = scorer.score(cleaned_text, ringkasan_tfid)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"ROUGE-1:\")\n",
    "print(f\"  Precision: {scores['rouge1'].precision:.4f}\")\n",
    "print(f\"  Recall: {scores['rouge1'].recall:.4f}\")\n",
    "print(f\"  F1 Score: {scores['rouge1'].fmeasure:.4f}\")\n",
    "\n",
    "print(\"ROUGE-2:\")\n",
    "print(f\"  Precision: {scores['rouge2'].precision:.4f}\")\n",
    "print(f\"  Recall: {scores['rouge2'].recall:.4f}\")\n",
    "print(f\"  F1 Score: {scores['rouge2'].fmeasure:.4f}\")\n",
    "\n",
    "print(\"ROUGE-L:\")\n",
    "print(f\"  Precision: {scores['rougeL'].precision:.4f}\")\n",
    "print(f\"  Recall: {scores['rougeL'].recall:.4f}\")\n",
    "print(f\"  F1 Score: {scores['rougeL'].fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.1739\n",
      "  F1 Score: 0.2963\n",
      "ROUGE-2:\n",
      "  Precision: 0.9639\n",
      "  Recall: 0.1660\n",
      "  F1 Score: 0.2832\n",
      "ROUGE-L:\n",
      "  Precision: 0.8690\n",
      "  Recall: 0.1511\n",
      "  F1 Score: 0.2575\n"
     ]
    }
   ],
   "source": [
    "# Membuat objek scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Menghitung skor\n",
    "scores = scorer.score(cleaned_text, ringkasan_wo2vec)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"ROUGE-1:\")\n",
    "print(f\"  Precision: {scores['rouge1'].precision:.4f}\")\n",
    "print(f\"  Recall: {scores['rouge1'].recall:.4f}\")\n",
    "print(f\"  F1 Score: {scores['rouge1'].fmeasure:.4f}\")\n",
    "\n",
    "print(\"ROUGE-2:\")\n",
    "print(f\"  Precision: {scores['rouge2'].precision:.4f}\")\n",
    "print(f\"  Recall: {scores['rouge2'].recall:.4f}\")\n",
    "print(f\"  F1 Score: {scores['rouge2'].fmeasure:.4f}\")\n",
    "\n",
    "print(\"ROUGE-L:\")\n",
    "print(f\"  Precision: {scores['rougeL'].precision:.4f}\")\n",
    "print(f\"  Recall: {scores['rougeL'].recall:.4f}\")\n",
    "print(f\"  F1 Score: {scores['rougeL'].fmeasure:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:\n",
      "  Precision: 1.0000\n",
      "  Recall: 0.1739\n",
      "  F1 Score: 0.2963\n",
      "ROUGE-2:\n",
      "  Precision: 0.9639\n",
      "  Recall: 0.1660\n",
      "  F1 Score: 0.2832\n",
      "ROUGE-L:\n",
      "  Precision: 0.8690\n",
      "  Recall: 0.1511\n",
      "  F1 Score: 0.2575\n"
     ]
    }
   ],
   "source": [
    "# Membuat objek scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Menghitung skor\n",
    "scores = scorer.score(cleaned_text, ringkasan_bert)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(\"ROUGE-1:\")\n",
    "print(f\"  Precision: {scores['rouge1'].precision:.4f}\")\n",
    "print(f\"  Recall: {scores['rouge1'].recall:.4f}\")\n",
    "print(f\"  F1 Score: {scores['rouge1'].fmeasure:.4f}\")\n",
    "\n",
    "print(\"ROUGE-2:\")\n",
    "print(f\"  Precision: {scores['rouge2'].precision:.4f}\")\n",
    "print(f\"  Recall: {scores['rouge2'].recall:.4f}\")\n",
    "print(f\"  F1 Score: {scores['rouge2'].fmeasure:.4f}\")\n",
    "\n",
    "print(\"ROUGE-L:\")\n",
    "print(f\"  Precision: {scores['rougeL'].precision:.4f}\")\n",
    "print(f\"  Recall: {scores['rougeL'].recall:.4f}\")\n",
    "print(f\"  F1 Score: {scores['rougeL'].fmeasure:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
