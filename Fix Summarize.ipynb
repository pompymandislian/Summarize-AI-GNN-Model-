{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PoSianturi\\AppData\\Local\\Temp\\ipykernel_19868\\2214985227.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-01</td>\n",
       "      <td>kenapa cuaca hari ini enak banget, ya kayaknya pas banget buat ke taman, biar bisa menikmati udara segar sambil curhat sama temanteman. . saya lagi belajar python, wow, seru banget kalo ada tips atau trik buat newbie kayak saya, pls share ya bantuin banget . cuaca sejuk hari ini bikin pengin ngumpul sambil ngopi. bagaimana kalo kita ketemuan dan ceritacerita seru pasti asyik banget tadi lihat orang jogging, langsung terinspirasi bikin pengin jadi lebih aktif. kamu juga suka olahraga, kan hari ini saya baca buku pengembangan diri, wow, makin semangat ada buku lain yang wajib dibaca pengin belajar terus malam ini rencananya mau makan pizzza pizza itu makanan fav saya kamu juga suka pizza topping favorit kamu apa baru pulang dari pasar, beli bahan fresh buat masak. akhir pekan ini mau masak apa, ya sudah ada resep seru buat dicoba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-08</td>\n",
       "      <td>hari ini pas banget buat piknik siapa mau ikut rasanya asyik kalo bisa ngumpul dan bersenangsenang di luar weekend kemarin seru habis saya habisin waktu bareng keluarga. bagaimana weekend kamu pasti ada cerita seru juga kemarin menonton film yang gila asik ada rekomendasi film lain yang seru pengin menonton yang bikin mengakak atau bawa perasaan begitu. rencananya mau coba resep baru buat sarapan. kamu ada resep enak yang gampang bantuin dong, butuh ideide fresh cuaca hangat ini bikin pengin jalanjalan ke tempat baru ada tempat seru yang kamu tau saya suka explore hal baru tadi ketemu teman lama, nostalgia kapan kita bisa ketemu lagi kangen sama momen seru yang kita lewatin bareng cek ini , tempat ini super cocok buat piknik sudah pernah kesana pasti seru buat relax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-15</td>\n",
       "      <td>baca info lebih lanjut di tentang pengembangan diri banyak hal seru yang bisa dipelajari kemarin menonton film terbaru di , dan itu benarbenar seru plot twistnya bikin saya shocked, pengin menonton lagi sekarang lagi mencoba hidup lebih sehat dengan olahraga. kamu olahraga apa, nih pengennya bugar kayak kamu eh, kenapa kamu enggak balas pesan saya penasaran deh kamu sibuk sama apa. ayo, jangan lupa balas, ya makan es krim pas cuaca panas itu enak siapa mau ajak saya makan es krim saya suka semua rasa, hehe kapan kita bisa kumpul lagi sudah kangen banget sama kamu rasanya lama banget enggak ketemu, yuk kita atur waktu kemarin menonton film yang bikin baper siapa mau menonton bareng lagi minggu depan yuk kita atur jadwalnya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  \\\n",
       "0  2024-10-01   \n",
       "1  2024-10-08   \n",
       "2  2024-10-15   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \n",
       "0  kenapa cuaca hari ini enak banget, ya kayaknya pas banget buat ke taman, biar bisa menikmati udara segar sambil curhat sama temanteman. . saya lagi belajar python, wow, seru banget kalo ada tips atau trik buat newbie kayak saya, pls share ya bantuin banget . cuaca sejuk hari ini bikin pengin ngumpul sambil ngopi. bagaimana kalo kita ketemuan dan ceritacerita seru pasti asyik banget tadi lihat orang jogging, langsung terinspirasi bikin pengin jadi lebih aktif. kamu juga suka olahraga, kan hari ini saya baca buku pengembangan diri, wow, makin semangat ada buku lain yang wajib dibaca pengin belajar terus malam ini rencananya mau makan pizzza pizza itu makanan fav saya kamu juga suka pizza topping favorit kamu apa baru pulang dari pasar, beli bahan fresh buat masak. akhir pekan ini mau masak apa, ya sudah ada resep seru buat dicoba  \n",
       "1                                                                 hari ini pas banget buat piknik siapa mau ikut rasanya asyik kalo bisa ngumpul dan bersenangsenang di luar weekend kemarin seru habis saya habisin waktu bareng keluarga. bagaimana weekend kamu pasti ada cerita seru juga kemarin menonton film yang gila asik ada rekomendasi film lain yang seru pengin menonton yang bikin mengakak atau bawa perasaan begitu. rencananya mau coba resep baru buat sarapan. kamu ada resep enak yang gampang bantuin dong, butuh ideide fresh cuaca hangat ini bikin pengin jalanjalan ke tempat baru ada tempat seru yang kamu tau saya suka explore hal baru tadi ketemu teman lama, nostalgia kapan kita bisa ketemu lagi kangen sama momen seru yang kita lewatin bareng cek ini , tempat ini super cocok buat piknik sudah pernah kesana pasti seru buat relax  \n",
       "2                                                                                                              baca info lebih lanjut di tentang pengembangan diri banyak hal seru yang bisa dipelajari kemarin menonton film terbaru di , dan itu benarbenar seru plot twistnya bikin saya shocked, pengin menonton lagi sekarang lagi mencoba hidup lebih sehat dengan olahraga. kamu olahraga apa, nih pengennya bugar kayak kamu eh, kenapa kamu enggak balas pesan saya penasaran deh kamu sibuk sama apa. ayo, jangan lupa balas, ya makan es krim pas cuaca panas itu enak siapa mau ajak saya makan es krim saya suka semua rasa, hehe kapan kita bisa kumpul lagi sudah kangen banget sama kamu rasanya lama banget enggak ketemu, yuk kita atur waktu kemarin menonton film yang bikin baper siapa mau menonton bareng lagi minggu depan yuk kita atur jadwalnya  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database connection details\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "\n",
    "def load_data_from_db(query):\n",
    "    \"\"\"Load data from PostgreSQL database using a SQL query.\"\"\"\n",
    "    try:\n",
    "        # Establish a connection to the database\n",
    "        with psycopg2.connect(\n",
    "            host=db_host,\n",
    "            port=db_port,\n",
    "            dbname=db_name,\n",
    "            user=db_user,\n",
    "            password=db_password\n",
    "        ) as conn:\n",
    "            # Execute the query and load data into a DataFrame\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            \n",
    "        return df\n",
    "    except psycopg2.OperationalError as e:\n",
    "        print(f\"OperationalError connecting to the database: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the database: {e}\")\n",
    "\n",
    "    # Return an empty DataFrame in case of error\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# load data\n",
    "query = 'SELECT * FROM \"jpku\".\"summarize_ai_7days\";'\n",
    "df = load_data_from_db(query)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Text (GNN Model)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PoSianturi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'xx_ent_wiki_sm' (3.7.0) was trained with spaCy v3.7.0 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_add_pool, global_max_pool\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "# Load multilingual model for spaCy\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "class SimpleGNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Graph Neural Network (GNN) model that combines \n",
    "    Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features for each node.\n",
    "        out_channels (int): Number of output features for each node.\n",
    "        activation (str): Activation function to use ('relu' or 'tanh').\n",
    "        aggregation (str): Aggregation method for pooling ('mean', 'sum', or 'max').\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, activation='relu', aggregation='mean'):\n",
    "        super(SimpleGNN, self).__init__()\n",
    "        \n",
    "        # First layer: GCN\n",
    "        self.conv1 = GCNConv(in_channels, 16)\n",
    "        \n",
    "        # Second layer: GAT\n",
    "        self.conv2 = GATConv(16, 16, heads=1)\n",
    "        \n",
    "        # Third layer: GAT\n",
    "        self.conv3 = GATConv(16, 16, heads=1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.final_conv = GCNConv(16, out_channels)\n",
    "        \n",
    "        # Store activation and aggregation type\n",
    "        self.activation = activation\n",
    "        self.aggregation = aggregation\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Forward pass for the GNN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Node feature matrix.\n",
    "            edge_index (torch.Tensor): Graph connectivity in COO format.\n",
    "            batch (torch.Tensor): Batch vector for batching.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output node features after passing through the GNN layers.\n",
    "        \"\"\"\n",
    "        # First layer\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Second layer\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Third layer\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = self._apply_activation(h)\n",
    "\n",
    "        # Global aggregation function\n",
    "        if self.aggregation == 'mean':\n",
    "            h = global_mean_pool(h, batch)\n",
    "            \n",
    "        elif self.aggregation == 'sum':\n",
    "            h = global_add_pool(h, batch)\n",
    "            \n",
    "        elif self.aggregation == 'max':\n",
    "            h = global_max_pool(h, batch)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation type. Choose 'mean', 'sum', or 'max'.\")\n",
    "\n",
    "        # Output\n",
    "        h = self.final_conv(h, edge_index)\n",
    "        return h\n",
    "\n",
    "    def _apply_activation(self, h):\n",
    "        \"\"\"\n",
    "        Applies the specified activation function.\n",
    "\n",
    "        Args:\n",
    "            h (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Activated tensor.\n",
    "        \"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(h)\n",
    "        \n",
    "        elif self.activation == 'tanh':\n",
    "            return F.tanh(h)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation type. Choose 'relu' or 'tanh'.\")\n",
    "\n",
    "class TextSummarizer:\n",
    "    \"\"\"\n",
    "    A text summarization class that utilizes BERT for embeddings \n",
    "    and builds a graph-based representation of text.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (BertTokenizer): BERT tokenizer for sentence encoding.\n",
    "        model (BertModel): BERT model for generating embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_sentences(text):\n",
    "        \"\"\"\n",
    "        Extracts sentences from the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of sentences extracted from the text.\n",
    "        \"\"\"\n",
    "        return [sent.text.strip() for sent in nlp(text).sents]\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_entities(text):\n",
    "        \"\"\"\n",
    "        Extracts named entities from the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of tuples containing entities and their labels.\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_entity(entity):\n",
    "        \"\"\"\n",
    "        Normalizes an entity by converting it to lowercase and stripping whitespace.\n",
    "\n",
    "        Args:\n",
    "            entity (str): The entity to normalize.\n",
    "\n",
    "        Returns:\n",
    "            str: The normalized entity.\n",
    "        \"\"\"\n",
    "        return entity.lower().strip()\n",
    "\n",
    "    def build_graph(self, sentences):\n",
    "        \"\"\"\n",
    "        Builds a graph from the list of sentences, where sentences and entities are nodes \n",
    "        and relationships are edges.\n",
    "\n",
    "        Args:\n",
    "            sentences (list): A list of sentences.\n",
    "\n",
    "        Returns:\n",
    "            networkx.Graph: The constructed graph.\n",
    "        \"\"\"\n",
    "        graph = nx.Graph()\n",
    "        entity_map = {}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            entities = self.extract_entities(sentence)\n",
    "            if not entities:\n",
    "                continue\n",
    "\n",
    "            normalized_entities = [(self.normalize_entity(ent[0]), ent[1]) for ent in entities]\n",
    "\n",
    "            # Add node for the sentence\n",
    "            graph.add_node(sentence, type='sentence')\n",
    "\n",
    "            for entity in normalized_entities:\n",
    "                # Add node for the entity\n",
    "                graph.add_node(entity[0], type='entity')\n",
    "\n",
    "                # Add edge between sentence and entity\n",
    "                graph.add_edge(sentence, entity[0], type='contains')\n",
    "\n",
    "                if entity[0] in entity_map:\n",
    "                    entity_map[entity[0]].add(sentence)\n",
    "                else:\n",
    "                    entity_map[entity[0]] = {sentence}\n",
    "\n",
    "            # Connect entities to each other\n",
    "            for i in range(len(normalized_entities)):\n",
    "                for j in range(i + 1, len(normalized_entities)):\n",
    "                    graph.add_edge(normalized_entities[i][0], normalized_entities[j][0], type='related')\n",
    "\n",
    "            # Add syntactic relationships\n",
    "            doc = nlp(sentence)\n",
    "            for token in doc:\n",
    "                if token.dep_ in {'nsubj', 'dobj', 'pobj'}:\n",
    "                    graph.add_edge(sentence, token.text, type='syntactic')\n",
    "\n",
    "                # Identify and connect temporal entities\n",
    "                if token.ent_type_ in {'DATE', 'TIME', 'EVENT'}:\n",
    "                    graph.add_node(token.text, type='temporal')\n",
    "                    graph.add_edge(sentence, token.text, type='temporal')\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def extract_bert_embeddings(self, sentences):\n",
    "        \"\"\"\n",
    "        Extracts BERT embeddings for the given sentences.\n",
    "\n",
    "        Args:\n",
    "            sentences (list): A list of sentences.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of numpy arrays representing the embeddings for each sentence.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for sentence in sentences:\n",
    "            inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state[0][0].numpy())\n",
    "        return embeddings\n",
    "\n",
    "    def select_important_words(self, selected_sentences, embeddings):\n",
    "        \"\"\"\n",
    "        Selects important words from the selected sentences based on embeddings.\n",
    "\n",
    "        Args:\n",
    "            selected_sentences (list): List of selected sentences.\n",
    "            embeddings (list): Corresponding embeddings for the selected sentences.\n",
    "\n",
    "        Returns:\n",
    "            list: List of important words from the selected sentences.\n",
    "        \"\"\"\n",
    "        important_words = []\n",
    "        for i, sentence in enumerate(selected_sentences):\n",
    "            sentence_embedding = embeddings[i]\n",
    "            words = sentence.split()\n",
    "            word_embeddings = self.extract_bert_embeddings(words)\n",
    "\n",
    "            distances = [np.linalg.norm(word_emb - sentence_embedding) for word_emb in word_embeddings]\n",
    "            closest_word_index = np.argmin(distances)\n",
    "            important_words.append(words[closest_word_index])\n",
    "            \n",
    "        return important_words\n",
    "\n",
    "    def summarizer(self, text, num_sentences=5, graph_algorithm='rank'):\n",
    "        \"\"\"\n",
    "        Summarizes the input text by extracting important sentences.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to summarize.\n",
    "            num_sentences (int): Number of sentences to include in the summary.\n",
    "            graph_algorithm (str): Algorithm to use for ranking sentences ('rank', 'closeness', 'hits', or 'degree').\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing a list of selected sentences and a list of important words.\n",
    "        \"\"\"\n",
    "        # Ekstrak kalimat dari teks input\n",
    "        sentences = self.extract_sentences(text)\n",
    "        \n",
    "        # Bangun graf dari kalimat yang diekstrak\n",
    "        graph = self.build_graph(sentences)\n",
    "\n",
    "        # Jika graf tidak memiliki node, kembalikan hasil kosong\n",
    "        if graph.number_of_nodes() == 0:\n",
    "            return [], \"\"\n",
    "\n",
    "        # Ekstrak embedding BERT untuk kalimat\n",
    "        embeddings = self.extract_bert_embeddings(sentences)\n",
    "        \n",
    "        # Inisialisasi skor untuk setiap kalimat\n",
    "        sentence_scores = {sentence: 0 for sentence in sentences}\n",
    "\n",
    "        if graph_algorithm == 'rank':\n",
    "            scores = nx.pagerank(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(scores.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        elif graph_algorithm == 'closeness':\n",
    "            scores = nx.degree_centrality(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = scores.get(sentence, 0)\n",
    "\n",
    "        elif graph_algorithm == 'hits':\n",
    "            hubs, authorities = nx.hits(graph)\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(authorities.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        elif graph_algorithm == 'degree':\n",
    "            scores = dict(graph.degree())\n",
    "            for sentence in sentences:\n",
    "                sentence_scores[sentence] = sum(scores.get(entity[0], 0) for entity in self.extract_entities(sentence))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid graph_algorithm. Use 'rank', 'closeness', 'hits', or 'degree'.\")\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            embedding_score = np.linalg.norm(embeddings[i])  # Hitung norma dari embedding\n",
    "            sentence_scores[sentence] += embedding_score * 0.1  # Tambahkan kontribusi embedding ke skor\n",
    "\n",
    "        # Urutkan kalimat berdasarkan skor\n",
    "        ranked_sentences = sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        selected_sentences = []  # Daftar kalimat terpilih\n",
    "        selected_indices = set()  # Set untuk melacak indeks yang sudah dipilih\n",
    "\n",
    "        for sentence, score in ranked_sentences:\n",
    "            idx = sentences.index(sentence)  # Ambil indeks kalimat\n",
    "            \n",
    "            # Pastikan tidak ada kalimat bertetangga yang dipilih\n",
    "            if not any(abs(idx - si) < 2 for si in selected_indices):\n",
    "            \n",
    "                selected_sentences.append(sentence)  \n",
    "            \n",
    "                if len(selected_sentences) == num_sentences: \n",
    "                    break\n",
    "\n",
    "        # Pilih kata penting dari kalimat terpilih\n",
    "        important_words = self.select_important_words(selected_sentences, embeddings)\n",
    "\n",
    "        summary = \" \".join(selected_sentences).strip('\"')\n",
    "        \n",
    "        # Kembalikan kalimat terpilih dan kata penting\n",
    "        return summary, important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_extraction = TextSummarizer()\n",
    "summary_gnn, important_words_extraction = summarizer_extraction.summarizer(cleaned_text, num_sentences=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Telemedicine memungkinkan akses baik terhadap layanan kesehatan, mengurangi beban sistem kesehatan, mempercepat waktu respon terhadap masalah kesehatan. Dengan memanfaatkan machine learning, dokter memperoleh wawasan tentang riwayat kesehatan pasien merancang perawatan personal. Media sosial telah menjadi sumber informasi utama bagi banyak orang, meskipun sering kali informasi disebarkan tidak selalu akurat terpercaya. Sistem kerja hybrid juga mulai populer, mana karyawan memilih bekerja kantor rumah sesuai kebutuhan mereka. Selain itu, munculnya fenomena hoaks disinformasi media sosial menuntut pengguna cermat memilah informasi. Dengan semua perkembangan ini, jelas bahwa teknologi terus memainkan peran sentral kehidupan seharihari kita. Oleh karena itu, penting bagi individu perusahaan memahami cara melindungi informasi menggunakan langkahlangkah keamanan tepat, enkripsi data pelatihan keamanan siber bagi karyawan. Misalnya, platform Zoom Microsoft Teams memungkinkan rapat virtual mendukung kolaborasi lintas waktu lokasi. Banyak perusahaan kini menerapkan sistem kerja jarak jauh, memungkinkan karyawan bekerja rumah. Dengan adanya teknologi telemedicine, pasien kini berkonsultasi dokter jarak jauh tanpa harus datang langsung rumah sakit.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kesehatan.',\n",
       " 'pasien',\n",
       " 'disebarkan',\n",
       " 'populer,',\n",
       " 'disinformasi',\n",
       " 'kehidupan',\n",
       " 'melindungi',\n",
       " 'kolaborasi',\n",
       " 'menerapkan',\n",
       " 'berkonsultasi']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words_extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Teknologi telah mengubah cara berkomunikasi era digital signifikan. Media sosial memainkan peran penting menyebarkan informasi, memberikan platform bagi individu berbagi pemikiran. Banyak orang menghabiskan waktu berjamjam platform Facebook, Instagram, Twitter.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load model dan tokenizer\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def summarize_text(text, max_length=100):\n",
    "    \"\"\"\n",
    "    Function to summarize the input text using BART.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to summarize.\n",
    "    max_length (int): Maximum length of the summary.\n",
    "\n",
    "    Returns:\n",
    "    str: The summarized text.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "summary_bart = summarize_text(cleaned_text)\n",
    "summary_bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_score(text1, text2):\n",
    "    '''Calculate Cosine Similarity between two texts.'''\n",
    "    # Pastikan input adalah string\n",
    "    text1 = ' '.join(text1) if isinstance(text1, list) else text1\n",
    "    text2 = ' '.join(text2) if isinstance(text2, list) else text2\n",
    "\n",
    "    # Convert both texts into vectors using CountVectorizer\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Calculate and return the cosine similarity between the two vectors\n",
    "    cosine_sim = cosine_similarity(vectors)\n",
    "    \n",
    "    return cosine_sim[0][1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine GNN 0.7586218172151918\n",
      "Cosine Bert 0.4232334732850844\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity_score\n",
    "cosine_gnn = cosine_similarity_score(cleaned_sentences, summary_gnn)\n",
    "cosine_bart = cosine_similarity_score(cleaned_sentences, summary_bart)\n",
    "\n",
    "print('Cosine GNN', cosine_gnn)\n",
    "print('Cosine Bert', cosine_bart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_summary(reference_summary, generated_summary):\n",
    "    \"\"\"\n",
    "    Evaluates the generated summary against the reference summary using ROUGE.\n",
    "\n",
    "    Args:\n",
    "        reference_summary (str): The original summary that serves as a reference.\n",
    "        generated_summary (str): The summary generated by the model.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing ROUGE-2 scores formatted to two decimal places.\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_summary, generated_summary)\n",
    "\n",
    "    # Format nilai ROUGE-2 ke dua angka di belakang koma\n",
    "    return {\n",
    "        'rouge2': {\n",
    "            'precision': round(scores['rouge2'].precision, 2),\n",
    "            'recall': round(scores['rouge2'].recall, 2),\n",
    "            'fmeasure': round(scores['rouge2'].fmeasure, 2)\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GNN**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge2': {'precision': 0.94, 'recall': 0.3, 'fmeasure': 0.46}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = cleaned_text\n",
    "generated_gnn = summary_gnn\n",
    "\n",
    "rouge_scores_gnn = evaluate_summary(reference, generated_gnn)\n",
    "\n",
    "rouge_scores_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge2': {'precision': 0.93, 'recall': 0.06, 'fmeasure': 0.11}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = cleaned_text\n",
    "generated_bart = summary_bart\n",
    "rouge_scores_bart = evaluate_summary(reference, generated_bart)\n",
    "\n",
    "rouge_scores_bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collact Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summarize GNN</th>\n",
       "      <th>Important Words GNN</th>\n",
       "      <th>Summarize Bart</th>\n",
       "      <th>Metric Rough Bart</th>\n",
       "      <th>Metric Rough GNN</th>\n",
       "      <th>Metric Cosine GNN</th>\n",
       "      <th>Metric Cosine Bart</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Telemedicine memungkinkan akses baik terhadap ...</td>\n",
       "      <td>kesehatan., pasien, disebarkan, populer,, disi...</td>\n",
       "      <td>Teknologi telah mengubah cara berkomunikasi er...</td>\n",
       "      <td>{'precision': 0.93, 'recall': 0.06, 'fmeasure'...</td>\n",
       "      <td>{'precision': 0.94, 'recall': 0.3, 'fmeasure':...</td>\n",
       "      <td>0.758622</td>\n",
       "      <td>0.423233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Summarize GNN  \\\n",
       "0  Telemedicine memungkinkan akses baik terhadap ...   \n",
       "\n",
       "                                 Important Words GNN  \\\n",
       "0  kesehatan., pasien, disebarkan, populer,, disi...   \n",
       "\n",
       "                                      Summarize Bart  \\\n",
       "0  Teknologi telah mengubah cara berkomunikasi er...   \n",
       "\n",
       "                                   Metric Rough Bart  \\\n",
       "0  {'precision': 0.93, 'recall': 0.06, 'fmeasure'...   \n",
       "\n",
       "                                    Metric Rough GNN  Metric Cosine GNN  \\\n",
       "0  {'precision': 0.94, 'recall': 0.3, 'fmeasure':...           0.758622   \n",
       "\n",
       "   Metric Cosine Bart  \n",
       "0            0.423233  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Menggabungkan semua kata penting menjadi satu string\n",
    "combined_important_words = ', '.join(important_words_extraction)\n",
    "\n",
    "data = {\n",
    "    'Summarize GNN': summary_gnn, \n",
    "    'Important Words GNN': combined_important_words,  \n",
    "    'Summarize Bart' : summary_bart,\n",
    "    'Metric Rough Bart' : rouge_scores_bart,\n",
    "    'Metric Rough GNN' : rouge_scores_gnn,\n",
    "    'Metric Cosine GNN' : cosine_gnn,\n",
    "    'Metric Cosine Bart' : cosine_bart\n",
    "    \n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Index secara default sudah mulai dari 0\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
